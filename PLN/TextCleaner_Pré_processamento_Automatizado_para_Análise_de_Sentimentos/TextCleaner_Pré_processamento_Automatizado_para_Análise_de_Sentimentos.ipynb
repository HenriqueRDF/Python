{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGfg-WXPxHjs",
        "outputId": "aaae08f3-bf8c-456a-ec8b-bd5f5e30d36b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Importa√ß√£o de bibliotecas\n",
        "# ------------------------\n",
        "# NLTK: Biblioteca para processamento de linguagem natural\n",
        "# re: Biblioteca para opera√ß√µes com express√µes regulares\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download dos recursos necess√°rios do NLTK\n",
        "# -----------------------------------------\n",
        "# stopwords: Lista de palavras irrelevantes (ex: \"a\", \"o\", \"de\")\n",
        "# punkt: Tokenizador para divis√£o do texto em palavras\n",
        "# punkt_tab: Tokenizador para portugu√™s\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Carrega o tokenizador em portugu√™s\n",
        "# ----------------------------------\n",
        "# Necess√°rio para garantir que o tokenizador funcione corretamente com textos em PT-BR\n",
        "nltk.data.load('tokenizers/punkt/PY3/portuguese.pickle')\n",
        "\n",
        "# Defini√ß√£o do texto de exemplo\n",
        "# -----------------------------\n",
        "# Lista de frases para demonstra√ß√£o do pr√©-processamento\n",
        "# Cont√©m exemplos com emojis e pontua√ß√£o para teste\n",
        "texto = [\n",
        "    \"√≥timo produto! Adorei a qualidade üòä\",\n",
        "    \"P√©ssima experi√™ncia. Nunca mais compro aqui! üòí\",\n",
        "    \"Entrega r√°pida e eficiente. Muito satisfeito!\"\n",
        "]\n",
        "\n",
        "# Fun√ß√£o de pr√©-processamento\n",
        "# ---------------------------\n",
        "def preproc_texto(texto):\n",
        "\n",
        "    #Realiza o pr√©-processamento de texto para an√°lise de sentimentos.\n",
        "    #\n",
        "    #Etapas:\n",
        "    #1. Converte para min√∫sculas\n",
        "    #2. Remove pontua√ß√£o e emojis\n",
        "    #3. Tokeniza o texto\n",
        "    #4. Filtra stopwords\n",
        "    #\n",
        "    #Par√¢metros:\n",
        "    #   texto (str): Texto a ser processado\n",
        "\n",
        "    #Retorna:\n",
        "    #    list: Lista de tokens limpos\n",
        "\n",
        "    texto = texto.lower()  # Min√∫sculo\n",
        "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Remove pontua√ß√£o e emojis\n",
        "    tokens = word_tokenize(texto, language='portuguese')  # Especificar o idioma\n",
        "    stop_words = set(stopwords.words('portuguese'))\n",
        "    tokens_filtrados = [palavra for palavra in tokens if palavra not in stop_words]\n",
        "    return tokens_filtrados\n",
        "\n",
        "# Aplicando o pr√©-processamento\n",
        "textos_processados = [preproc_texto(frase) for frase in texto]\n",
        "\n",
        "print(\"Textos pr√©-processados: \", textos_processados)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrW7LgVlxLMT",
        "outputId": "4593570a-2286-4de6-e0ec-e4df5ec83d6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Textos pr√©-processados:  [['√≥timo', 'produto', 'adorei', 'qualidade'], ['p√©ssima', 'experi√™ncia', 'nunca', 'compro', 'aqui'], ['entrega', 'r√°pida', 'eficiente', 'satisfeito']]\n"
          ]
        }
      ]
    }
  ]
}