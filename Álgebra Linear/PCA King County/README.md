# üè° Otimiza√ß√£o de Regress√£o com PCA: Redu√ß√£o de Dimensionalidade no Mercado Imobili√°rio de King County üìä

![Python Badge](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scikit-learn Badge](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![NumPy Badge](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Status](https://img.shields.io/badge/Status-Conclu√≠do-green)

## üéØ Objetivo do Projeto

Este projeto demonstra a aplica√ß√£o pr√°tica da **An√°lise de Componentes Principais (PCA)** para otimizar um modelo de Regress√£o Linear. O foco foi encontrar o *trade-off* ideal entre a complexidade do modelo (n√∫mero de *features*) e sua performance preditiva no mercado de im√≥veis de King County (Seattle).

A principal contribui√ß√£o √© a an√°lise de **√Ålgebra Linear** (Autovalores e Autovetores) para reduzir o espa√ßo vetorial de dados sem comprometer significativamente a precis√£o.

## üíæ Dataset

O conjunto de dados utilizado √© amplamente conhecido na √°rea de Machine Learning para Regress√£o.

* **Nome:** House Sales in King County, USA
* **Fonte:** [Kaggle](https://www.kaggle.com/datasets/harlfoxem/housesalesprediction)
* **Descri√ß√£o:** Cont√©m dados de vendas de casas na √°rea de King County, Washington, incluindo Seattle, entre maio de 2014 e maio de 2015. A vari√°vel alvo (`target`) √© o pre√ßo (`price`).
* **Uso:** O dataset √© acessado diretamente no notebook via `kagglehub` para garantir a reprodutibilidade.

## ‚öôÔ∏è Metodologia T√©cnica e Matem√°tica

A solu√ß√£o foi dividida nas seguintes etapas, com destaque para a aplica√ß√£o da matem√°tica subjacente:

1.  **Pr√©-processamento:** Limpeza de dados categ√≥ricos (One-Hot Encoding em `zipcode`) e **Escalonamento** (`StandardScaler`) das features. *O escalonamento √© obrigat√≥rio antes do PCA.*
2.  **Modelo Baseline:** Treinamento de uma Regress√£o Linear no dataset original (87 features) para estabelecer a m√©trica de compara√ß√£o.
3.  **An√°lise PCA (O Cora√ß√£o da √Ålgebra Linear):**
    * C√°lculo dos **Autovalores e Autovetores** da Matriz de Covari√¢ncia.
    * An√°lise da **Varian√ßa Acumulada Explicada** via *Scree Plot*.
4.  **Redu√ß√£o de Dimensionalidade:** Sele√ß√£o do n√∫mero √≥timo de componentes para reter **95% da vari√¢ncia**.
5.  **Modelo Otimizado:** Treinamento do mesmo modelo de Regress√£o Linear no dataset transformado pelo PCA.

## üìä Resultados e An√°lise do Trade-off

A an√°lise da vari√¢ncia acumulada (Scree Plot) indicou que a maior parte da informa√ß√£o dos dados estava concentrada em um subespa√ßo de menor dimens√£o.

| M√©trica | Baseline (87 Features) | Otimizado (73 Componentes) |
| :--- | :--- | :--- |
| **Dimensionalidade Reduzida** | N/A | **16.09%** |
| **R¬≤ Score** | **0.8081** | **0.7814** |
| **RMSE (Erro M√©dio)** | R\$ 170.329,55 | R\$ 181.809,50 |

### **Conclus√£o:**

O PCA permitiu uma redu√ß√£o de dimensionalidade de **16.09%** (de 87 para 73 features), o que se traduz em um modelo mais r√°pido e eficiente em termos de custo computacional. A perda de $2.67\%$ no $R^2$ (de 0.8081 para 0.7814) √© um *trade-off* aceit√°vel e muitas vezes prefer√≠vel em cen√°rios de produ√ß√£o de larga escala, onde a velocidade de infer√™ncia √© cr√≠tica.

## üõ†Ô∏è Tecnologias Utilizadas

* **Linguagem:** Python 3.x
* **Manipula√ß√£o de Dados:** `pandas`, `numpy`
* **Machine Learning/Estat√≠stica:** `scikit-learn` (`LinearRegression`, `StandardScaler`, `PCA`)
* **Visualiza√ß√£o:** `matplotlib`, `seaborn`
